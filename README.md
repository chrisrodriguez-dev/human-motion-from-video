## Overview

This project processes 2D video input (from a file or webcam) to generate a 3D skeleton-based human motion reconstruction using deep learning and physics-informed modeling. It is adapted from PHC (Physics-based Human-centric Motion Generation) for use in real-time demo environments.

# Human Motion from Video

This project is a research-focused adaptation of the [PHC (Physics-based Human-Centric Motion)](https://github.com/ZhengyiLuo/PHC) model. It generates 3D human motion from video input or real-time webcam feed using deep learning and physics-informed modeling.

I am currently working with this codebase as part of an undergraduate research project, and this repository serves to document my environment setup, custom testing, and potential contributions to the PHC pipeline.

---

## Overview

The original PHC model provides a framework to predict realistic, physics-aware human motion from monocular video. In this adaptation, I focus on:
- Replicating the results in a controlled local environment
- Running and documenting the webcam demo
- Learning and understanding the underlying model architecture
- Preparing the system for potential extensions or integration into other research tools

---


